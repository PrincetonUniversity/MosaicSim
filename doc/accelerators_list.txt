 ================================
  List of supported TF functions
 ================================

 ----------------------------
  Accelerators and software
 ----------------------------

  - NN layers

    - conv2d

    - conv3d

    - dense

  - NN activations

    - relu

    - prelu

    - tanh

    - sigmoid

    - batch normalization

  - NN misc

    - bias add

    - local response normalization

  - Pooling

    - max pooling

    - avg pooling

  - Matrix operations

    - matrix multiplication

  - Element-wise Matrix Operations

    - add

    - subtract

    - multiply

    - divide

    - add scalar

    - subtract scalar

    - multiply scalar

    - divide scalar

 ----------------------------
  Software only
 ----------------------------

  - NN layers

    - conv2dbackpropinput

    - conv2dbackpropfilter

  - NN activations

    - elu

    - softmax

    - sigmoid gradient

    - tanh gradient

  - NN pooling

    - min pooling

  - NN misc

    - bias add gradient

    - log loss

    - reduction

 =============================
  Python signatures: user API
 =============================

  - conv2d -> tf.keras.layers.Conv2D

  - conv3d -> tf.keras.layers.Conv3D

  - dense -> tf.keras.layers.Dense

  - relu -> tf.keras.layers.ReLU

  - prelu -> tf.keras.layers.PReLU

  - tanh -> tf.keras.activations.tanh 

  - sigmoid -> tf.keras.activations.sigmoid

  - batch normalization -> tf.keras.layers.BatchNormalization

  - bias add -> tf.keras.backend.bias_add

  - local response normalization -> tf.nn.local_response_normalization 

  - max pooling -> tf.keras.layers.MaxPool2D

  - average pooling -> tf.keras.layers.AveragePooling2D

  - tensor multiplication -> tf.linalg.matmul

  - tensor addition -> tf.math.add, tf.keras.layers.add?

  - tensor subtraction -> tf.math.subtract, tf.keras.layers.subtract

  - tensor element-wise multiplication -> tf.math.multiply, tf.keras.layers.multiply

  - tensor element-wise division -> tf.math.divide

  - dot product -> tf.keras.layers.dot

  - tensor multiplication by a scalar -> tf.math.scalar_mul

  - [!] tensor addition, subtraction and division by a scalar -> ???

  - elu -> tf.keras.layers.ELU

  - softmax -> tf.keras.layers.Softmax

  - log loss -> tf.losses.log_loss

  - reduction -> tf.losses.Reduction

  - [!] conv2dbackpropinput -> tensorflow::ops::Conv2DBackpropInput

  - [!] conv2sbackpropfilter -> tensorflow::ops::Conv2DBackpropFilter

  - [!] tanh gradient -> ???

  - [!] sigmoid gradient -> ???

  - min pooling -> ???

  - bias add gradient -> tensorflow::ops::BiasAddGrad
